{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f044b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import DQN\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "from copy import deepcopy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a4bde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreAndTerminateWrapper(gym.Wrapper):\n",
    "  ''''\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  :param max_steps: (int) Max number of steps per episode\n",
    "  '''\n",
    "  def __init__(self, env):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(StoreAndTerminateWrapper, self).__init__(env)\n",
    "    self.max_steps = 200\n",
    "    # Counter of steps per episode\n",
    "    self.current_step = 0\n",
    "    self.mem = []\n",
    "    self.TotalReward = 0.0 \n",
    "    self.env = env\n",
    "    self.first_state = 0\n",
    "    self.first_obs = 0\n",
    "    self.prev_obs = 0 \n",
    "    self.states_list = []\n",
    "    self.info = {}\n",
    "  \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    # Reset the counter\n",
    "    self.current_step = 0\n",
    "    obs =self.env.reset()\n",
    "    self.TotalReward = 0.0\n",
    "    self.first_obs = obs\n",
    "    return obs\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    In this function we store the initial state as well as the memory of the agent\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    if self.current_step == 0: #store initial state\n",
    "      self.prev_obs = self.first_obs\n",
    "      self.first_state = deepcopy(self.env)\n",
    "      self.states_list.append(self.first_state)\n",
    "    # print(\"t\",self.env.state[0],\"reward\",self.TotalReward)\n",
    "    # if self.env.state[0]==-1.2:\n",
    "    #   print(\"-1.2\")\n",
    "    #   obs = self.reset()\n",
    "    #   reward = -200\n",
    "    #   done = True\n",
    "    #   return obs, reward, done, False\n",
    "    self.current_step += 1\n",
    "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "    done = terminated or truncated\n",
    "    self.TotalReward += reward\n",
    "    self.mem.append(tuple((self.prev_obs,action)))\n",
    "    self.prev_obs = obs\n",
    "    if self.current_step >= self.max_steps:\n",
    "      done = True\n",
    "      # Update the info dict to signal that the limit was exceeded\n",
    "    if obs[0]<=-1.2:\n",
    "      done = True\n",
    "      reward = -201 - self.TotalReward\n",
    "      self.TotalReward =-200\n",
    "      # print(\"fff\",reward)\n",
    "    if done:\n",
    "      self.mem.append(tuple(('done',self.TotalReward)))\n",
    "    self.info['mem'] = self.mem\n",
    "    self.info['state'] = self.states_list\n",
    "    # self.mem.append(tuple(obs,action))\n",
    "    return obs, reward, done, info\n",
    "\n",
    "  def set_state(self, state):\n",
    "    \"\"\"\n",
    "    :param state: initial state of the episode\n",
    "    :return: environment is updated and observations is returned\n",
    "    \"\"\"\n",
    "    self.env = deepcopy(state)\n",
    "    obs = np.array(list(self.env.unwrapped.state))\n",
    "    self.current_step = 0\n",
    "    self.TotalReward = 0.0\n",
    "    self.first_obs = obs\n",
    "    return obs\n",
    "\n",
    "def abstract_state_general(model,state1,d):\n",
    "  if type(state1) == str:\n",
    "    if state1 == 'done':\n",
    "      return 'end'\n",
    "  q_values = model.step_model.step([state1])\n",
    "  return tuple([ceil(q_value/d) for q_value in q_values[1][0]])\n",
    "\n",
    "def Abstract_classes(ep,abstraction_d,model):\n",
    "  d=abstraction_d\n",
    "  abs_states1=[]\n",
    "  for episode in ep:\n",
    "    for state,action in episode:\n",
    "      abs_st = abstract_state_general(model,state,d)\n",
    "      if abs_st == 'end':\n",
    "        continue\n",
    "      abs_states1.append(abs_st)\n",
    "  unique1=list(set(abs_states1))\n",
    "  uni1 = np.array(unique1)\n",
    "  a=len(abs_states1)\n",
    "  b=len(set(abs_states1))\n",
    "  print(\"abstract states:\",b)\n",
    "  print(\"Concrete states\",a)\n",
    "  print(\"ratio\",b/a)\n",
    "  return unique1,uni1\n",
    "\n",
    "def ML_first_representation_func_based(Abs_d,functional_func,reward_func,model,input_episodes,unique1):\n",
    "  \"\"\"\n",
    "  TO-DO : fix epsilon and threshold\n",
    "  \"\"\"\n",
    "  d = Abs_d\n",
    "  data1_x_b=[]\n",
    "  data1_y_b= [] \n",
    "  data1_y_f_b = []\n",
    "  for i, episode in enumerate(input_episodes):\n",
    "    record = np.zeros(len(unique1))\n",
    "    temp_flag = False\n",
    "    for state, action in episode:\n",
    "      ab = abstract_state_general(model,state,d)\n",
    "      if ab == 'end':\n",
    "        assert not temp_flag, f'Episode data problem, two terminations in one episode. Episode number{i}'\n",
    "        temp_flag = True\n",
    "        # print(action)\n",
    "        # print(functional_func(episode))\n",
    "        if functional_func(episode):\n",
    "          data1_y_f_b.append(1)\n",
    "        else:\n",
    "          data1_y_f_b.append(0)\n",
    "        if reward_func(episode):\n",
    "          data1_y_b.append(1)\n",
    "        else:\n",
    "          data1_y_b.append(0)\n",
    "        # print(\"end\\n\\n\\n\")\n",
    "        # print(len(data1_y_b),\"len(input_episodes)\",len(input_episodes))\n",
    "        continue\n",
    "        # print(state[0])\n",
    "      ind = unique1.index(ab)\n",
    "      record[ind] = 1\n",
    "      # print(state, action)\n",
    "      assert len(data1_y_b)<len(input_episodes), \"assert\"\n",
    "      # if you want the frequency go with the next line \n",
    "      # record[ind] += 1\n",
    "    data1_x_b.append(record)\n",
    "\n",
    "  return data1_x_b, data1_y_b, data1_y_f_b\n",
    "\n",
    "def report(model2,x_train, y_train,x_test, y_test):\n",
    "  print(\"********************** reporting the result of the model **************************\")\n",
    "  print('The score for train data is {0}'.format(model2.score(x_train,y_train)))\n",
    "  print('The score for test data is {0}'.format(model2.score(x_test,y_test)))\n",
    "\n",
    "\n",
    "  predictions_train = model2.predict(x_train)\n",
    "  predictions_test = model2.predict(x_test)\n",
    "\n",
    "  print(\"\\n\\n--------------------------------------recall---------------------------------\")\n",
    "\n",
    "  print('the test recall for the class yes is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=1)))\n",
    "  print('the test recall for the class no is {0}'.format(metrics.recall_score(y_test,predictions_test, pos_label=0)))\n",
    "\n",
    "  print('the training recall for the class yes is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=1)))\n",
    "  print('the training recall for the class no is {0}'.format(metrics.recall_score(y_train,predictions_train, pos_label=0)))\n",
    "\n",
    "\n",
    "  print(\"\\n\\n--------------------------------------precision------------------------------\")\n",
    "\n",
    "\n",
    "  print('the test precision for the class yes is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=1)))\n",
    "  print('the test precision for the class no is {0}'.format(metrics.precision_score(y_test,predictions_test, pos_label=0)))\n",
    "\n",
    "  print('the training precision for the class yes is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=1)))\n",
    "  print('the training precision for the class no is {0}'.format(metrics.precision_score(y_train,predictions_train, pos_label=0)))\n",
    "\n",
    "  print(\"\\n\\n\")\n",
    "  print(classification_report(y_test, predictions_test, target_names=['NO ','yes']))\n",
    "\n",
    "  tn, fp, fn, tp = confusion_matrix(y_test, predictions_test).ravel()\n",
    "  specificity = tn / (tn+fp)\n",
    "  print(\"\\n\\nspecifity :\",specificity)\n",
    "  print(\"\\n\\n--------------------------------------confusion----------------------------\")\n",
    "  CM = metrics.confusion_matrix(y_test, predictions_test)\n",
    "  print(\"The confusion Matrix:\")\n",
    "  print(CM)\n",
    "  print('the accuracy score in {0}\\n\\n'.format(accuracy_score(y_test, predictions_test)))\n",
    "  print(\"********************** plotting the confusion matrix & ROC curve **************************\")\n",
    "  ConfusionMatrixDisplay(model2, x_test, y_test)\n",
    "  metrics.plot_roc_curve(model2, x_test, y_test) \n",
    "  plt.show()\n",
    "\n",
    "# write function for load\n",
    "\n",
    "\n",
    "def random_test_2(model, env, Num):\n",
    "    # Gymnasium v0.29+的reset()返回(obs, info)元组\n",
    "    obs, info = env.reset()  # 解包获得观测值\n",
    "    counter = 1\n",
    "    episode_reward = 0.0\n",
    "    end = Num  # 初始化end防止未完成任何episode的情况\n",
    "    \n",
    "    for i in range(Num):\n",
    "        # 只传递obs部分给predict\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        # Gymnasium v0.29+的step()返回5个值（包括truncated）\n",
    "        obs, reward, done, info = env.step(action)  \n",
    "        episode_reward += reward\n",
    "        if done:  # 检查episode结束\n",
    "            counter += 1\n",
    "            end = i\n",
    "            episode_reward = 0.0\n",
    "            obs, info = env.reset()  # 重置时再次解包\n",
    "    \n",
    "    # 处理记忆数据\n",
    "    last_index = -1  # 默认值\n",
    "    \n",
    "    # 逆向查找最后一个'done'标记\n",
    "    for u in range(1, len(env.info['mem'])+1):\n",
    "        current_item = env.info['mem'][-u]\n",
    "        # 处理可能出现的数组情况\n",
    "        if isinstance(current_item[0], (np.ndarray, list)):\n",
    "            # 如果是数组，检查是否包含'done'\n",
    "            if 'done' in current_item[0]:\n",
    "                last_index = -u\n",
    "                break\n",
    "        elif current_item[0] == 'done':\n",
    "            last_index = -u\n",
    "            break\n",
    "    \n",
    "    # 计算切片范围\n",
    "    remaining_steps = Num - end if end != Num else 0\n",
    "    randomtest = env.info['mem'][last_index:-remaining_steps] if remaining_steps > 0 else env.info['mem'][last_index:]\n",
    "    \n",
    "    # 处理状态数据\n",
    "    ran_state = []\n",
    "    if counter > 1:\n",
    "        try:\n",
    "            ran_state = env.info['state'][-counter+1:-1]\n",
    "        except (IndexError, KeyError):\n",
    "            ran_state = []\n",
    "    \n",
    "    return randomtest, ran_state\n",
    "\n",
    "def fix_testing(testing_episodes,testing_states,Env2):\n",
    "  buffer =[] \n",
    "  episodes_set = []\n",
    "  j=0\n",
    "  for i in range(len(testing_episodes)):\n",
    "    if testing_episodes[i][0] == 'done':\n",
    "      if i == 0:\n",
    "        continue\n",
    "      buffer.append(testing_episodes[i])\n",
    "      episodes_set.append(buffer)\n",
    "      buffer=[]\n",
    "    else:\n",
    "      buffer.append(testing_episodes[i])\n",
    "      # np.array(mtc_wrapped.set_state(qq[0]),dtype=\"float32\")\n",
    "  if not (episodes_set[0][0][0]==np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
    "    del testing_states[0]\n",
    "  if not (episodes_set[0][0][0]==np.array(Env2.set_state(testing_states[0]),dtype=\"float32\")).all():\n",
    "    assert False, 'problem in starting states'\n",
    "  if len(episodes_set)!=len(testing_states):\n",
    "    del testing_states[-1]\n",
    "  if len(episodes_set)!=len(testing_states):\n",
    "    assert False, 'problem in data prepration'\n",
    "  return episodes_set , testing_states\n",
    "\n",
    "def is_functional_fault(episode):\n",
    "  epsilon = 0.1\n",
    "  env = mtc_wrapped\n",
    "  reward = episode[-1][1]\n",
    "  last_state = episode[-2][0][0]\n",
    "  if last_state<(env.low[0]+epsilon) and reward == -200:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_reward_fault(episode):\n",
    "  RF_threshold = -180\n",
    "  reward = episode[-1][1]\n",
    "  # print(len(episode))\n",
    "  if reward<RF_threshold and len(episode)>200:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def is_functional_fault_last_state(last_step,done_step):\n",
    "  epsilon = 0.1\n",
    "  env = mtc_wrapped\n",
    "  assert done_step[0]=='done', \"Wrong input!\"\n",
    "  reward = done_step[1]\n",
    "  last_state = last_step[0][0]\n",
    "  if last_state<(env.low[0]+epsilon) and reward == -200:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_reward_fault_last_state(last_step,done_step):\n",
    "  RF_threshold = -180\n",
    "  assert done_step[0]=='done', \"Wrong input!\"\n",
    "  reward = done_step[1]\n",
    "  last_state = last_step[0][0]\n",
    "  # print(len(episode))\n",
    "  if reward<RF_threshold and not is_functional_fault_last_state(last_step,done_step):\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def load_p(name):\n",
    "  with open(f'/content/drive/MyDrive/MC/{name}.pickle', 'rb') as file2:\n",
    "    to_what = pickle.load(file2)\n",
    "  return to_what\n",
    "def local_load_p(name):\n",
    "  with open(f'c:/Users/Student/Desktop/Data/{name}', 'rb') as file2:\n",
    "    to_what = pickle.load(file2)\n",
    "  return to_what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3076c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(episode,model, d, unique5):\n",
    "  \"\"\"\n",
    "  thid function takes the concrete episodes and returns the encoded episodes \n",
    "  based on the presence and absence of the individuals  \n",
    "  :param 'episode': input episode\n",
    "  :param 'model': RL model\n",
    "  :param 'd': abstraction level = 1\n",
    "  :param 'unique5': abstract classes \n",
    "  :return: encoded episodse based on the presence and absence\n",
    "\n",
    "  \"\"\"\n",
    "  d=d\n",
    "  record = np.zeros(len(unique5))\n",
    "  for state, action in episode:\n",
    "    ab = abstract_state_general(model,state,d)\n",
    "    if ab == 'end':\n",
    "      continue\n",
    "    if ab in unique5:\n",
    "      ind = unique5.index(ab)\n",
    "      record[ind] = 1\n",
    "  return [record]\n",
    "\n",
    "def episode_player(episodes,d, abs_classes, model, monitor) -> list:\n",
    "  ''' This function replays the episodes and returns the risk of each step in each episode\n",
    "  :param 'episodes': input episodes\n",
    "  :param 'd': abstraction level \n",
    "  :param 'abs_classes': abstract classes\n",
    "  :param 'model': RL model\n",
    "  :param 'monitor': ML model\n",
    "  :return: risk of each step in each episode\n",
    "  \n",
    "  '''\n",
    "  episodes_risk=[]\n",
    "  for episode in episodes:\n",
    "    risk_array=[]\n",
    "    for step in range(len(episode)-1):\n",
    "      monitoring_data = translator(episode[:step],model,d,abs_classes)\n",
    "      Risk = monitor.predict_proba(monitoring_data)\n",
    "      risk_array.append(Risk[0][1])\n",
    "    episodes_risk.append(risk_array)\n",
    "  return episodes_risk\n",
    "\n",
    "def single_episode_player(episode,d, abs_classes, model, monitor) -> list:\n",
    "  ''' This function replays one episodes and returns the risk of each step in episode\n",
    "  :param 'episode': input episode\n",
    "  :param 'd': abstraction level\n",
    "  :param 'abs_classes': abstract classes\n",
    "  :param 'model': RL model\n",
    "  :param 'monitor': ML model\n",
    "  :return: risk of each step in episode\n",
    "  '''\n",
    "  risk_array=[]\n",
    "  for step in range(len(episode)-1):\n",
    "    monitoring_data = translator(episode[:step],model,d,abs_classes)\n",
    "    Risk = monitor.predict_proba(monitoring_data)\n",
    "    risk_array.append(Risk[0][1])\n",
    "  return risk_array\n",
    "\n",
    "def line_plot(data):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    for i in range(len(data)): \n",
    "        plt.plot( [i for i in range(len(data[i]))], data[i], label = f\"Episode {i}\")\n",
    "    # plt.plot(y, x, label = \"line 2\")\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_positions(episodes):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    for i in range(len(episodes)):\n",
    "        position =[]\n",
    "        position_arr =[]\n",
    "        for j in range(len(episodes[i])-1):\n",
    "            position.append(episodes[i][j][0][0])\n",
    "        position_arr.append(position)\n",
    "        plt.plot([i for i in range(len(position))], position, label = f\"Episode {i}\")\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_velocity(episodes):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    for i in range(len(episodes)):\n",
    "        velocity =[]\n",
    "        velocity_arr =[]\n",
    "        for j in range(len(episodes[i])-1):\n",
    "            velocity.append(episodes[i][j][0][1])\n",
    "        velocity_arr.append(velocity)\n",
    "        plt.plot([i for i in range(len(velocity))], velocity, label = f\"Episode {i}\")\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def position_extractor(episode):\n",
    "    position =[]\n",
    "    for i in range(len(episode)-1):\n",
    "        position.append(episode[i][0][0])\n",
    "    return position\n",
    "    \n",
    "def velocity_extractor(episode):\n",
    "    velocity =[]\n",
    "    for i in range(len(episode)-1):\n",
    "        velocity.append(episode[i][0][1])\n",
    "    return velocity\n",
    "\n",
    "\n",
    "def Plot_all(data, params,save=False,show=True,data_chunk=0,path='Plots/v2'):\n",
    "    '''plot risk and position snd velocity in one figure with 3 subplots\n",
    "    '''\n",
    "    d,unique1,model,RF_FF_1rep = params\n",
    "    fig, axs = plt.subplots(3,figsize=(20, 18))\n",
    "    for i in range(len(data)):\n",
    "        axs[0].plot([i for i in range(len(data[i])-1)], single_episode_player(data[i],d,unique1,model,RF_FF_1rep), label = f\"Episode {i}\")\n",
    "        axs[1].plot([i for i in range(len(data[i])-1)], position_extractor(data[i]), label = f\"Episode {i}\")\n",
    "        axs[2].plot([i for i in range(len(data[i])-1)], velocity_extractor(data[i]), label = f\"Episode {i}\")\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title('Risk')\n",
    "    axs[1].set_title('Position')\n",
    "    axs[2].set_title('Velocity')\n",
    "    axs[1].legend()\n",
    "    axs[2].legend()\n",
    "    axs[0].set_ylim(-0.1,1.1)\n",
    "    current_time = datetime.now()\n",
    "    ID = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    if save:\n",
    "        fig.savefig(f'{path}/RPV_C{data_chunk}_{ID}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d96dae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "Dataset_path = \"D:\\\\code\\\\RLtest\\\\1.zip\"\n",
    "mtc = gym.make('MountainCar-v0')\n",
    "mtc_wrapped = StoreAndTerminateWrapper(mtc)\n",
    "model = DQN('MlpPolicy',env=mtc_wrapped, verbose=1)\n",
    "model = model.load(Dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT,RTS = random_test_2(model,mtc_wrapped,300_000)\n",
    "FRT,FRTS = fix_testing(RT,RTS,mtc_wrapped)\n",
    "#save FRT and FRTS as pickle\n",
    "training_size = 2200\n",
    "assert len(FRT) >= training_size\n",
    "Dataset_path2 = \"D:\\code\\SMARLA\\Dataset\\Mountain-Car\\\\\"\n",
    "with open(f'{Dataset_path2}/Random_episodes/FRT_training.pickle', 'wb') as file2:\n",
    "    pickle.dump(FRT[:training_size], file2)\n",
    "with open(f'{Dataset_path2}/Random_episodes/FRTS_training.pickle', 'wb') as file2:\n",
    "    pickle.dump(FRTS[:training_size], file2)\n",
    "\n",
    "RF=0\n",
    "FF=0\n",
    "Buff_reward = 0\n",
    "Buff_len = 0\n",
    "for test_episode in FRT:\n",
    "    Buff_reward += test_episode[-1][1]\n",
    "    Buff_len += (len(test_episode)-1)\n",
    "    if is_functional_fault(test_episode):\n",
    "        FF+=1\n",
    "    if is_reward_fault(test_episode):\n",
    "        RF+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdpfuzz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
